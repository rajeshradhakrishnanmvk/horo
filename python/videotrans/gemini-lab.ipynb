{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ml\\horo\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "GOOGLE_API_KEY=os.environ['GOOGLE_API_KEY']\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_1_file = genai.upload_file(path='.\\\\LLM-AndreiKarpathy\\\\audio\\\\segment_9.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Listen carefully to the following audio file. Extract the text from the audio file and write it down.\"\n",
    "model = genai.GenerativeModel('models/gemini-1.5-flash')\n",
    "response = model.generate_content([prompt, segment_1_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That basically the next word prediction task you might think is a very simple objective but it's actually a pretty powerful objective because it forces you to learn a lot about the world inside the parameters of the neural network. So here I took a random webpage um at the time when I was making this talk. I just grabbed it from the main page of Wikipedia and it was uh about Ruth Handler. And so think about being the neural network and you're given some amount of words and try to predict the next word in a sequence. Well in this case I'm highlighting here in red some of the words that would contain a lot of information. And so for example in a in if your objective is to predict the next word presumably your parameters have to learn a lot of this knowledge. You have to know about Ruth and Handler and when she was born and when she died uh who she was uh what she has done and so on. And so in the task of next word prediction you're learning a ton about the world and all this knowledge is being compressed into the weights uh the parameters. P \""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
